{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.1232, 0.1434\n",
      "Q3: 0.2781, 0.2365\n",
      "Q5: 0.0965, 0.1371\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# A1.1\n",
    "# Q1: Tree x3 then x1 then x2 all on the left.\n",
    "# Q2: Yes, for this example, there exists a smaller tree that perfectly classify all training examples. This tree is not found by ID3 since it is a greedy algorithm (the attribute selection at a node is done without regard to the selection at children nodes).\n",
    "\n",
    "# Q3:\n",
    "# Gain = Entropy before – Sum of proportion  (Chosen / all) * entropy of chosen\n",
    "def Gain(S, xi):\n",
    "    sum_S = sum(S)\n",
    "    p_pos, p_neg = S[0]/sum_S, S[1]/sum_S\n",
    "    entropy_S = -p_pos * math.log2(p_pos) -p_neg * math.log2(p_neg)\n",
    "    \n",
    "    sumvalue = 0\n",
    "    for v in range(len(xi)):\n",
    "        mult = sum(xi[v]) / sum_S    # |S_v| / |S|\n",
    "        sum_Xiv = sum(xi[v])\n",
    "        p_posv, p_negv = xi[v][0]/sum_Xiv, xi[v][1]/sum_Xiv\n",
    "        \n",
    "        if (p_posv == 0): entropy_Sv = -p_negv * math.log2(p_negv)\n",
    "        elif (p_negv == 0): entropy_Sv = -p_posv * math.log2(p_posv)\n",
    "        else: entropy_Sv = -p_posv * math.log2(p_posv) -p_negv * math.log2(p_negv)\n",
    "        sumvalue += (mult*entropy_Sv)\n",
    "    return entropy_S - sumvalue\n",
    "\n",
    "S = (10, 10)\n",
    "xi = [(8, 2), (2, 8)]\n",
    "first = Gain(S, xi)\n",
    "xi = [(10, 6), (0, 4)]\n",
    "second = Gain(S, xi)\n",
    "print(\"Q3: {0:.4f}, {1:.4f}\".format(first, second))\n",
    "\n",
    "# Q4: [8+,2−] (left)  [2+,8−] (right)\n",
    "\n",
    "# Q5:\n",
    "S = (100, 10)\n",
    "xi = [[80, 2], [20, 8]]\n",
    "first = Gain(S, xi)\n",
    "xi = [[100, 6], [0, 4]]\n",
    "second = Gain(S, xi)\n",
    "print(\"Q5: {0:.4f}, {1:.4f}\".format(first, second))\n",
    "\n",
    "# Q6: [100+,6−] (left)  [0+,4−] (right)\n",
    "\n",
    "# Q7: information gain is sensitive to class cost imbalance.\n",
    "#     with c⊕=10 and c⊖=1\n",
    "\n",
    "# Q8: c^k^d   and then ID3: Sum(k^i * (d-i))\n",
    "\n",
    "# Q9: The decision boundaries are parallel to the axes. Thus, the instance space is split into rectangles.\n",
    "#     The number of regions depends on the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_nodes = 445\n",
      "train_accur = 1.0\n",
      "test_accur = 0.5008347245409015\n",
      "\n",
      "120.78 1.0 0.5037228714524207\n",
      "     Frac   Run  NodeCount  TrainAcc   TestAcc\n",
      "0    0.05   0.0       27.0       1.0  0.529215\n",
      "1    0.05   1.0       25.0       1.0  0.484140\n",
      "2    0.05   2.0       29.0       1.0  0.500835\n",
      "3    0.05   3.0       29.0       1.0  0.509182\n",
      "4    0.05   4.0       25.0       1.0  0.505843\n",
      "..    ...   ...        ...       ...       ...\n",
      "495  0.99  95.0      435.0       1.0  0.515860\n",
      "496  0.99  96.0      443.0       1.0  0.505843\n",
      "497  0.99  97.0      445.0       1.0  0.490818\n",
      "498  0.99  98.0      427.0       1.0  0.515860\n",
      "499  0.99  99.0      421.0       1.0  0.520868\n",
      "\n",
      "[500 rows x 5 columns]\n",
      "\n",
      "169.624 1.0 0.49546911519198666\n"
     ]
    }
   ],
   "source": [
    "# A1.2\n",
    "# Q1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier #https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "train_df = pd.read_csv(\"WineTrain.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"WineTest.csv\", index_col=0)\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "fit = classifier.fit(train_df.drop(\"Quality\", axis=1), train_df[\"Quality\"])\n",
    "\n",
    "y_train_pred = fit.predict(train_df.drop(\"Quality\", axis=1))\n",
    "y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "\n",
    "# Q2: nb_nodes, train_accur, test_accur.\n",
    "def accuracy(pred, true):\n",
    "    well = 0\n",
    "    total = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i]: \n",
    "            well += 1\n",
    "        total += 1\n",
    "    return well / total\n",
    "\n",
    "print(\"nb_nodes = {}\".format(classifier.tree_.node_count))\n",
    "print(\"train_accur = {}\".format(accuracy(y_train_pred, train_df[\"Quality\"].to_numpy())))\n",
    "print(\"test_accur = {}\".format(accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())))\n",
    "\n",
    "# Q3: Only if the data is consistent (no 2 examples with same features x1=x2 but different outcome y1≠y2).\n",
    "\n",
    "# Q4:\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "frame = pd.DataFrame(columns = [\"Run\", \"NodeCount\", \"TrainAcc\", \"TestAcc\"])\n",
    "\n",
    "data = {}\n",
    "for i in range(100):\n",
    "    data[i] = train_df.sample(int(len(train_df)*0.25), random_state=i)\n",
    "    \n",
    "    classifier = DecisionTreeClassifier(random_state=0)\n",
    "    fit = classifier.fit(data[i].drop(\"Quality\", axis=1), data[i][\"Quality\"])\n",
    "    y_train_pred = fit.predict(data[i].drop(\"Quality\", axis=1))\n",
    "    y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "    \n",
    "    temp = {'Run': i, 'NodeCount': classifier.tree_.node_count, 'TrainAcc': accuracy(y_train_pred, data[i][\"Quality\"].to_numpy()), 'TestAcc': accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())}\n",
    "    frame = frame.append(temp, ignore_index=True)\n",
    "\n",
    "mean_nb_nodes = np.mean(frame[\"NodeCount\"].to_numpy())\n",
    "mean_train_accur = np.mean(frame[\"TrainAcc\"].to_numpy())\n",
    "mean_test_accur = np.mean(frame[\"TestAcc\"].to_numpy())\n",
    "print()\n",
    "print(mean_nb_nodes, mean_train_accur, mean_test_accur)\n",
    "\n",
    "# Q5: \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "frame = pd.DataFrame(columns = [\"Frac\", \"Run\", \"NodeCount\", \"TrainAcc\", \"TestAcc\"])\n",
    "for frac in [0.05, 0.1, 0.2, 0.5, 0.99]:\n",
    "    for i in range(100):\n",
    "        data = train_df.sample(int(len(train_df)*frac), random_state=i)\n",
    "\n",
    "        classifier = DecisionTreeClassifier(random_state=0)\n",
    "        fit = classifier.fit(data.drop(\"Quality\", axis=1), data[\"Quality\"])\n",
    "        y_train_pred = fit.predict(data.drop(\"Quality\", axis=1))\n",
    "        y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "\n",
    "        temp = {'Frac': frac, 'Run': i, 'NodeCount': classifier.tree_.node_count, 'TrainAcc': accuracy(y_train_pred, data[\"Quality\"].to_numpy()), 'TestAcc': accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())}\n",
    "        frame = frame.append(temp, ignore_index=True)\n",
    "\n",
    "mean_nb_nodes = np.mean(frame[\"NodeCount\"].to_numpy())\n",
    "mean_train_accur = np.mean(frame[\"TrainAcc\"].to_numpy())\n",
    "mean_test_accur = np.mean(frame[\"TestAcc\"].to_numpy())\n",
    "print()\n",
    "print(mean_nb_nodes, mean_train_accur, mean_test_accur)\n",
    "\n",
    "# Q6: The test accuracy, as a function of the training set size, increases most when there are few training samples. This increase then slows down once the training sizes get larger. We may expect to have reached a plateau, meaning the accuracy won't vary much if more examples are added to the training data.\n",
    "#     The number of nodes of the trees increases with the training set size because more splits are needed to classify all training examples.\n",
    "#     The variance of the test accuracy decreases when the training set size increases because of the overlap between random data split increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning\n",
      "Avancement\n",
      "0.5742904841402338\n",
      "0.5826377295492488\n",
      "0.6110183639398998\n",
      "0.6310517529215359\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "Avancement\n",
      "0.6310517529215359\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_split': 115, 'min_samples_leaf': 35, 'max_features': None}\n",
      "Index(['FixedAcidity', 'VolatileAcidity', 'CitricAcid', 'ResidualSugar',\n",
      "       'Chlorides', 'FreeSulfurDioxide', 'TotalSulfurDioxide', 'Density', 'pH',\n",
      "       'Sulphates', 'Alcohol', 'Quality'],\n",
      "      dtype='object')\n",
      "[0.03849672 0.05746655 0.         0.00555444 0.         0.\n",
      " 0.15030912 0.0138968  0.         0.15572624 0.57855013]\n"
     ]
    }
   ],
   "source": [
    "# A1.3\n",
    "# Q1\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def accuracy(pred, true):\n",
    "    well = 0\n",
    "    total = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i]: \n",
    "            well += 1\n",
    "        total += 1\n",
    "    return well / total\n",
    "\n",
    "train_df = pd.read_csv(\"WineTrain.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"WineTest.csv\", index_col=0)\n",
    "\n",
    "temptemp = pd.DataFrame(columns = [\"min_samples_split\", \"NodeCount\", \"TrainAcc\", \"TestAcc\"])\n",
    "# print(\"Param\", \"NbrNode\", \"TrainAcc\", \"TestAcc\", \"AllAcc\")\n",
    "for j in [2, 120, 1200]:\n",
    "    frame = pd.DataFrame(columns = [\"min_samples_split\", \"NodeCount\", \"TrainAcc\", \"TestAcc\"])\n",
    "    for i in range(100):\n",
    "        data = train_df\n",
    "\n",
    "        classifier = DecisionTreeClassifier(random_state=0, min_samples_split=j)\n",
    "        fit = classifier.fit(data.drop(\"Quality\", axis=1), data[\"Quality\"])\n",
    "        y_train_pred = fit.predict(data.drop(\"Quality\", axis=1))\n",
    "        y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "\n",
    "        temp = {'min_samples_split': j, 'NodeCount': classifier.tree_.node_count, 'TrainAcc': accuracy(y_train_pred, data[\"Quality\"].to_numpy()), 'TestAcc': accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())}\n",
    "        frame = frame.append(temp, ignore_index=True)\n",
    "\n",
    "    mean_nb_nodes = np.mean(frame[\"NodeCount\"].to_numpy())\n",
    "    mean_train_accur = np.mean(frame[\"TrainAcc\"].to_numpy())\n",
    "    mean_test_accur = np.mean(frame[\"TestAcc\"].to_numpy()) \n",
    "    mean_all_accur = (mean_train_accur + mean_test_accur) / 2\n",
    "    temptemp = temptemp.append({'min_samples_split': j, 'NodeCount': mean_nb_nodes, 'TrainAcc': mean_train_accur, 'TestAcc': mean_test_accur}, ignore_index=True)\n",
    "    #print(j, round(mean_nb_nodes, 4), round(mean_train_accur, 4), round(mean_test_accur, 4), round(mean_all_accur, 4))\n",
    "frame = temptemp\n",
    "    \n",
    "# 600 et + c'esr pareil on a plus que 3 nodes et à 1200 on passe à 1 node\n",
    "# Entre 120 et 300, il n'y a rien d'interessant\n",
    "\n",
    "# Param NbrNode TrainAcc TestAcc AllAcc\n",
    "# 120 31.0 0.699 0.5843 0.6417 is best\n",
    "# 1200 1.0 0.505 0.399 0.452 is worst\n",
    "# 2 445.0 1.0 0.5008 0.7504 is inbetween, good overall because performs well on training\n",
    "\n",
    "# Q2: Using an adequate value for min_samples_split, the number of nodes, as a function of the training set size, of the pruned tree follows the same trend as the number of nodes of the complete tree. The main difference is a multiplicative factor.\n",
    "#     An adequatly pruned tree will have a higher test accuracy.\n",
    "#     Pruning the tree is not useful when using a very small number of training examples.\n",
    "#     Pruning the tree prevents overfitting.\n",
    "\n",
    "# Q3\n",
    "# # random_state in [42, 49, 69, 90, 120, 360, 18, 64, 128, 2, 74, 66, 33]:\n",
    "print(\"Beginning\")\n",
    "best_acc = 0.0\n",
    "best_param = {}\n",
    "for criterion in [\"gini\", \"entropy\"]:\n",
    "    for max_depth in [None, 3, 25, 50, 100, 300]:\n",
    "        print(\"Avancement\")\n",
    "        for min_samples_split in [115, 120, 125]:\n",
    "            for min_samples_leaf in [25, 30, 35, 40,]:\n",
    "                for max_features in [None, 2, 4, 6, 8, 11]:\n",
    "                    classifier = DecisionTreeClassifier(random_state=0, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_features=max_features)\n",
    "                    fit = classifier.fit(train_df.drop(\"Quality\", axis=1), train_df[\"Quality\"])\n",
    "                    y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "                    acc = accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())\n",
    "                    if best_acc < acc:\n",
    "                        print(acc)\n",
    "                        best_acc = acc\n",
    "                        best_param[\"criterion\"] = criterion\n",
    "                        best_param[\"max_depth\"] = max_depth\n",
    "                        best_param[\"min_samples_split\"] = min_samples_split\n",
    "                        best_param[\"min_samples_leaf\"] = min_samples_leaf\n",
    "                        best_param[\"max_features\"] = max_features\n",
    "print(best_acc)\n",
    "print(best_param)\n",
    "\n",
    "# 0.6310517529215359\n",
    "# {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 120, 'min_samples_leaf': 35, 'max_features': None}\n",
    "# 0.6260434056761269\n",
    "# {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 0.1, 'min_samples_leaf': 51, 'min_weight_fraction_leaf': 0.0, 'max_features': None, 'min_impurity_decrease': 0.0, 'class_weight': None}\n",
    "\n",
    "def my_best_tree(train):\n",
    "    classifier = DecisionTreeClassifier(random_state=0, criterion='gini', max_depth=None, min_samples_split=120, min_samples_leaf=35, max_features=None)\n",
    "    fit = classifier.fit(train.drop(\"Quality\", axis=1), train[\"Quality\"])\n",
    "    return classifier\n",
    "\n",
    "print(train_df.columns)\n",
    "print(my_best_tree(train_df).feature_importances_)\n",
    "# Alcohol, TotalSulfurDioxide, Sulphates, VolatileAcidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6143572621035058\n",
      "0.6410684474123539\n",
      "0.6494156928213689\n",
      "0.6510851419031719\n",
      "0.66110183639399\n",
      "0.66110183639399 {'n_estimators': 55, 'max_depth': 5, 'min_samples_split': 50, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "# A1.4\n",
    "\n",
    "# Q1\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def accuracy(pred, true):\n",
    "    well = 0\n",
    "    total = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == true[i]: \n",
    "            well += 1\n",
    "        total += 1\n",
    "    return well / total\n",
    "train_df = pd.read_csv(\"WineTrain.csv\", index_col=0)\n",
    "test_df = pd.read_csv(\"WineTest.csv\", index_col=0)\n",
    "\n",
    "wl = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "fit = wl.fit(train_df.drop(\"Quality\", axis=1), train_df[\"Quality\"])\n",
    "y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "acc = accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())\n",
    "# print(acc)  #0.5191986644407346\n",
    "\n",
    "# Q2\n",
    "# result = [0]*10\n",
    "# for j in range(10):\n",
    "#     y_test_preds = pd.DataFrame()\n",
    "#     print(\"Avancement\")\n",
    "#     for i in range(100):\n",
    "#         data = train_df.sample(len(train_df), replace=True, random_state=j*i)\n",
    "#         classifier = DecisionTreeClassifier(random_state=0, max_depth=2)\n",
    "#         fit = classifier.fit(data.drop(\"Quality\", axis=1), data[\"Quality\"])\n",
    "#         y_test_preds = y_test_preds.append([fit.predict(test_df.drop(\"Quality\", axis=1))])\n",
    "    \n",
    "#     y_test_pred = [0]*599\n",
    "#     for k in range(599):\n",
    "#         high = 0\n",
    "#         medium = 0\n",
    "#         low = 0\n",
    "#         for count in y_test_preds[k]:\n",
    "#             if count == 'high': high+=1\n",
    "#             elif count == 'medium': medium+=1\n",
    "#             elif count == 'low': low+=1\n",
    "#         best = max([high, medium, low])\n",
    "#         if best == high:\n",
    "#             y_test_pred[k] = 'high'\n",
    "#         elif best == medium:\n",
    "#             y_test_pred[k] = 'medium'\n",
    "#         else:\n",
    "#             y_test_pred[k] = 'low'\n",
    "#     result[j] = accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())\n",
    "\n",
    "# # print(result)\n",
    "# mean_test_accur = np.mean(result)\n",
    "\n",
    "# Q3\n",
    "# Using bagging, with the weak learner and an appropriate number of trees, will yield better results than using a single tree with optimally tuned min_samples_split (like the one you used in question 1 of A1.3).\n",
    "# Compared to our weak learner (a single tree), using bagging drastically improves the accuracy, even with only a few (e.g. 5) trees.\n",
    "# At some point, we reach an optimal number of trees, and adding more trees will reduce the accuracy due to overfitting.\n",
    "\n",
    "# Q4\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "best_acc = 0.0\n",
    "best_params = {}\n",
    "for n_estimators in [55, 60, 65, 300]:\n",
    "    for max_depth in [None, 5, 10, 20]:\n",
    "        for min_samples_split in [5, 25, 50, 160]:\n",
    "            for min_samples_leaf in [1, 5, 40]:\n",
    "                classifier = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
    "                fit = classifier.fit(train_df.drop(\"Quality\", axis=1), train_df[\"Quality\"])\n",
    "                y_test_pred = fit.predict(test_df.drop(\"Quality\", axis=1))\n",
    "                acc = accuracy(y_test_pred, test_df[\"Quality\"].to_numpy())\n",
    "                if best_acc < acc:\n",
    "                    print(acc)\n",
    "                    best_acc = acc\n",
    "                    best_params[\"n_estimators\"] = n_estimators\n",
    "                    best_params[\"max_depth\"] = max_depth\n",
    "                    best_params[\"min_samples_split\"] = min_samples_split\n",
    "                    best_params[\"min_samples_leaf\"] = min_samples_leaf\n",
    "print(best_acc, best_params)\n",
    "\n",
    "# Random State=0\n",
    "# 0.66110183639399 {'n_estimators': 60, 'max_depth': 5, 'min_samples_split': 50, 'min_samples_leaf': 1}\n",
    "\n",
    "my_best_forest = RandomForestClassifier(n_estimators=60, max_depth=10, min_samples_split=50, min_samples_leaf=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1]\n",
      "   0  1  2  3  4\n",
      "0  0  0  0  0  0\n",
      "0  1  2  3  4  5\n",
      "0  1  2  3  4  5\n",
      "0  1  2  3  4  5\n",
      "0  1  2  3  4  5\n"
     ]
    }
   ],
   "source": [
    "p = pd.DataFrame(np.array([[0, 0, 0, 0, 0]]))\n",
    "p = p.append([[1, 2, 3, 4, 5]])\n",
    "p = p.append([[1, 2, 3, 4, 5]])\n",
    "p = p.append([[1, 2, 3, 4, 5]])\n",
    "p = p.append([[1, 2, 3, 4, 5]])\n",
    "print(p[0].to_numpy())\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
