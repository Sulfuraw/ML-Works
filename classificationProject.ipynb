{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYv6tbRpUUCk"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "# Confusion matrix of best model on our validation data\n",
        "# Given code that can be copy-pasted\n",
        "def score_weight_class(bmi_pred, bmi_true, low, high):\n",
        "    tol=1\n",
        "    vpred = (bmi_pred >= low-tol) & (bmi_pred < high+tol)\n",
        "    vtrue = (bmi_true >= low) & (bmi_true < high)\n",
        "    if vtrue.sum() == 0:\n",
        "        # print(\"no true samples here\")\n",
        "        return 0\n",
        "    rmse = np.sqrt(((bmi_true[vtrue] - bmi_pred[vtrue]) ** 2).mean())\n",
        "    rmse = rmse/(high-low+tol)               # normalize rmse in interval\n",
        "    acc = (vpred & vtrue).sum()/vtrue.sum()  # % of accurate prediction for this bmi class\n",
        "    return rmse*(1-acc)\n",
        "\n",
        "# 0.0 = perfect (no error), the score changes indepentently from just the bmi_class predictions\n",
        "# i.e. if you're wrong and you predict 100kg, its better than if you're wrong and you predict 150kg\n",
        "# but if you predict correctly, it does not matter where you are in the range of the bmi class !\n",
        "def score_regression(ytrue, ypred, height):\n",
        "    bmi_pred = ypred/(height*height)\n",
        "    bmi_true = ytrue/(height*height)\n",
        "    scores = []\n",
        "    for bmi_low, bmi_high in zip([0, 18.5, 25, 30], [18.5, 25, 30, 100]):\n",
        "        scores.append(score_weight_class(bmi_pred, bmi_true, low=bmi_low, high=bmi_high))\n",
        "    return np.mean(scores)\n",
        "\n",
        "# Check freq of 4 classes\n",
        "# array containing indices of the samples belonging to the different classes\n",
        "def oversampling(df):\n",
        "    bmi_low = []\n",
        "    bmi_normal = []\n",
        "    bmi_high = []\n",
        "    bmi_very_high = []\n",
        "    for i in range(len(df)):\n",
        "        if (i in df[\"bmi\"]):\n",
        "            bmi = df[\"bmi\"][i]\n",
        "            if 0 <= bmi < 18.5:\n",
        "                bmi_low.append(i)\n",
        "            elif 18.5 <= bmi < 25:\n",
        "                bmi_normal.append(i)\n",
        "            elif 25 <= bmi < 30:\n",
        "                bmi_high.append(i)\n",
        "            elif 30 <= bmi < 100:\n",
        "                bmi_very_high.append(i)\n",
        "    # low: 14, normal: 157, high: 51, very_high: 22 (sum = 244, correct)\n",
        "\n",
        "    # Duplicate rare sample by checking bmi less represented\n",
        "    # we multiply each sample of under-represented class the same number of times to reach the closest number to the most frequent samples\n",
        "    # low: 14 -> *11 (154), normal: 157 -> X, high: 51 -> *3 (153), very_high: 22 -> *7 (154)\n",
        "    # new total will be 618\n",
        "\n",
        "    max_bmi = len(bmi_normal) # Len of bmi_normal because he's the most seen\n",
        "    duplicated_samples = df.loc[bmi_low, :]\n",
        "    for i in range(max_bmi//len(bmi_low)):  #since there is already the sample *1, you only add *10\n",
        "        df = pd.concat([df, duplicated_samples], ignore_index=True)\n",
        "    duplicated_samples = df.loc[bmi_high, :]\n",
        "    for i in range(max_bmi//len(bmi_high)):\n",
        "        df = pd.concat([df, duplicated_samples], ignore_index=True)\n",
        "\n",
        "    duplicated_samples = df.loc[bmi_very_high, :]\n",
        "    for i in range(max_bmi//len(bmi_very_high)):\n",
        "        df = pd.concat([df, duplicated_samples], ignore_index=True)\n",
        "    return df.drop([\"bmi\"], axis=1).to_numpy()\n",
        "\n",
        "# See correlation plot between feature and target\n",
        "def print_data(X, Y):\n",
        "    fig=plt.figure(figsize=(10, 10), dpi=90)\n",
        "    n_feats = len(X.columns)\n",
        "    for i, feat in enumerate(X.columns):\n",
        "        plt.subplot(n_feats//3+1,3,i+1)\n",
        "        plt.scatter(X[feat],Y , s=10)\n",
        "        plt.title(feat)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# See heatmap of correlation\n",
        "def heatmap(df):\n",
        "    plt.figure(figsize=(10,10))\n",
        "    cor = df.corr()\n",
        "    sns.heatmap(cor, annot=True, cmap='PiYG', center=0, vmin = -0.4, vmax = 0.4, linewidths=.5)\n",
        "    plt.show()\n",
        "\n",
        "def display_all_data(X):\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "        print(X)\n",
        "\n",
        "def KF_validation(X, Y, model, K, mse=2):\n",
        "    KF = KFold(n_splits=K, random_state=1, shuffle=True)\n",
        "    SUM = 0\n",
        "    # ite = 0\n",
        "    for train_index, test_index in KF.split(X):\n",
        "        X_train, X_test = X.loc[train_index][:].drop([\"bmi\"], axis=1).to_numpy(), X.loc[test_index][:].drop([\"bmi\"], axis=1).to_numpy()\n",
        "        Y_train, Y_test = Y.loc[train_index][:].drop([\"bmi\"], axis=1).to_numpy(), Y.loc[test_index][:].drop([\"bmi\"], axis=1).to_numpy()\n",
        "\n",
        "        # X_train = oversampling(X_train)\n",
        "        # Y_train = oversampling(Y_train)\n",
        "\n",
        "        # weight_ite = coef_weight[0:ite*25].append(coef_weight[(ite+1)*25:249])\n",
        "\n",
        "        # model.fit(X_train, np.ravel(Y_train))             #,sample_weight=weight_ite)\n",
        "        model.fit(X_train, Y_train)\n",
        "        if mse == 0:\n",
        "            SUM += mean_absolute_error(Y_test, model.predict(X_test))\n",
        "        elif mse == 1:\n",
        "            SUM += model.score(X_test, Y_test)\n",
        "        else: # mse==2\n",
        "            SUM += score_regression(Y_test, model.predict(X_test), X_test[:,2])\n",
        "            # SUM += score_regression(np.ravel(Y_test), model.predict(X_test), X_test[:,2])\n",
        "        # ite+=1\n",
        "    return SUM/K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhFU0By0UUCl"
      },
      "source": [
        "# Read csv and setup X1\n",
        "X1 = pd.read_csv(\"X1.csv\")\n",
        "Y1 = pd.read_csv(\"Y1.csv\", header=None, names=['weight'] )\n",
        "X2 = pd.read_csv(\"X2.csv\")\n",
        "\n",
        "X1 = X1.drop(\"Unnamed: 0\",axis=1)\n",
        "X2 = X2.drop(\"Unnamed: 0\",axis=1)\n",
        "\n",
        "#   Data Engineering\n",
        "# Step 1 : Re-encode : Make all values in number\n",
        "le_gender = preprocessing.LabelEncoder().fit([\"Male\", \"Female\"])\n",
        "le_yesno = preprocessing.LabelEncoder().fit([\"no\", \"yes\"])\n",
        "le_freq = preprocessing.LabelEncoder().fit([\"no\", \"Sometimes\", \"Frequently\", \"Always\"])\n",
        "le_transport = preprocessing.LabelEncoder().fit([\"Bike\", \"Walking\", \"Public_Transportation\", \"Motorbike\", \"Automobile\"])\n",
        "for categorical_feature, label_encoder in [(\"Gender\", le_gender), (\"family_history_with_overweight\", le_yesno), (\"FAVC\", le_yesno), (\"CAEC\", le_freq), (\"SMOKE\", le_yesno), (\"SCC\", le_yesno), (\"CALC\", le_freq), (\"MTRANS\", le_transport)]:\n",
        "    X1[categorical_feature] = label_encoder.transform(X1[categorical_feature])\n",
        "    X2[categorical_feature] = label_encoder.transform(X2[categorical_feature])\n",
        "\n",
        "    X1[categorical_feature] = X1[categorical_feature].astype(\"float\")\n",
        "    X2[categorical_feature] = X2[categorical_feature].astype(\"float\")\n",
        "\n",
        "# Drop not relevant features\n",
        "for i in [\"FAVC\", \"FCVC\", \"NCP\", \"CAEC\", \"SMOKE\", \"SCC\", \"FAF\", \"TUE\", \"CALC\"]:\n",
        "    X1 = X1.drop(i,axis=1)\n",
        "    X2 = X2.drop(i,axis=1)\n",
        "\n",
        "# Compute weight to give more impact on low seen data in model fitting (not used)\n",
        "coef_weight = []\n",
        "for i in range(len(X1[\"Age\"])):\n",
        "    weight_i = 1\n",
        "    if (not (15 <= X1[\"Age\"][i] < 37)):\n",
        "        weight_i += 1\n",
        "    if (not (1.55 < X1[\"Height\"][i] < 1.85)):\n",
        "        weight_i += 1\n",
        "    if (X1[\"MTRANS\"][i] == 1 or X1[\"MTRANS\"][i] == 2):\n",
        "        weight_i += 1\n",
        "    coef_weight.append(weight_i)\n",
        "\n",
        "# Data Augmenting\n",
        "# big datafram with all data (incl. weight)\n",
        "df = pd.concat([X1, Y1], axis=1)\n",
        "\n",
        "# Outlier removal\n",
        "# removed 6 outlier samples (26, 90, 91, 124, 214, 241) -> this means the df has now 244 rows\n",
        "df = df[(np.abs(scipy.stats.zscore(df)) < 3).all(axis=1)]\n",
        "\n",
        "# class imbalance\n",
        "# calculer les classes de BMI pour chaque personne\n",
        "bmi_col = []\n",
        "for i in range(250):\n",
        "    if (i in df[\"weight\"]):\n",
        "        weight = df[\"weight\"][i]\n",
        "        height = df[\"Height\"][i]\n",
        "        bmi = weight / (height*height)\n",
        "        bmi_col.append(bmi)\n",
        "    else:\n",
        "        bmi_col.append(-1)\n",
        "bmi_col = pd.DataFrame({\"bmi\": bmi_col})\n",
        "\n",
        "# Append bmi column to df\n",
        "# drop again previous outliers.\n",
        "df = pd.concat([df, bmi_col], axis=1)\n",
        "df = df.drop([26, 90, 91, 124, 214, 241], axis=0)\n",
        "df = df.dropna()\n",
        "df = df.reset_index()\n",
        "df = df.drop(\"index\", axis=1)\n",
        "\n",
        "Y1_np = df.loc[:][[\"weight\", \"bmi\"]]\n",
        "X1_np = df.drop([\"weight\"], axis=1)\n",
        "\n",
        "# print(KF_validation(X1_np, Y1_np, LinearRegression(), 10))\n",
        "# print(KF_validation(X1_np, Y1_np, KNeighborsRegressor(n_neighbors=2), 10))\n",
        "# print(KF_validation(X1_np, Y1_np, MLPRegressor(max_iter=1000), 10))\n",
        "# print(KF_validation(X1_np, Y1_np, RandomForestRegressor(max_depth=2, random_state=1), 10))\n",
        "\n",
        "# print_data(df.drop(\"weight\", axis=1), df[\"weight\"])\n",
        "# heatmap(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6KcwIFBUUCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0e20e8-572d-438a-9ee0-e18e39e5567f"
      },
      "source": [
        "# Model\n",
        "# K-Folding\n",
        "Y1_np = df.loc[:][[\"weight\", \"bmi\"]]\n",
        "X1_np = df.drop([\"weight\"], axis=1)\n",
        "\n",
        "#===============================================================================\n",
        "##### Custom Parameters Search #####\n",
        "\n",
        "### no grid search for linear model\n",
        "### best score : 0.12866988784982425\n",
        "# KF_LR = KF_validation(X1_np, Y1_np, LinearRegression(), 10)\n",
        "# print(\"LinearRegressor without parameters search : \"+ str(KF_LR))  # 0.12866988784982425\n",
        "\n",
        "### MLP parameter opti\n",
        "### best score : 0.15898470190027653 avec les best params : {'activation': 'identity', 'solver': 'adam', 'alpha': 1e-07, 'tol': 0.0001}\n",
        "# best_score = 1000\n",
        "# best_params = {}\n",
        "# for activation in [\"relu\", \"identity\", \"logistic\", \"tanh\"]:\n",
        "#     print(activation)\n",
        "#     for solver in [\"adam\"]:  # \"lbfgs\" , \"sgd\", \"adam\"\n",
        "#         for alpha in [ 0.00001, 0.000001, 0.0000001]:\n",
        "#             for tol in [1e-3, 1e-4, 1e-5]:\n",
        "#                 model = MLPRegressor(max_iter=800, activation=activation, solver=solver, alpha=alpha, tol=tol)\n",
        "#                 score = KF_validation(X1_np, Y1_np, model, 10)\n",
        "#                 if best_score > score:\n",
        "#                     best_score = score\n",
        "#                     best_params[\"activation\"] = activation\n",
        "#                     best_params[\"solver\"] = solver\n",
        "#                     best_params[\"alpha\"] = alpha\n",
        "#                     best_params[\"tol\"] = tol\n",
        "# print(\"best score : \" + str(best_score) + \" avec les best params : \" + str(best_params))\n",
        "# KF_MLP = KF_validation(X1_np, Y1_np, MLPRegressor(max_iter=1000), 10)\n",
        "# print(\"MLP without parameters search : \" + str(KF_MLP))  # 0.20188709269241106\n",
        "\n",
        "### KNN parameter opti\n",
        "### best score : 0.124841313131318 avec les best params : {'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 1, 'leaf_size': 10, 'n_neighbors': 15}\n",
        "best_score = 1000\n",
        "best_params = {}\n",
        "for weights in [\"uniform\", \"distance\"]:\n",
        "    for algorithm in [\"ball_tree\", \"kd_tree\"]:\n",
        "        for p in [1,2]:\n",
        "            for leaf_size in [10, 20, 30, 40, 50]:\n",
        "                for n_neighbors in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 20, 25]:\n",
        "                    model = KNeighborsRegressor(weights=weights, algorithm=algorithm, p=p, leaf_size=leaf_size, n_neighbors=n_neighbors)\n",
        "                    score = KF_validation(X1_np, Y1_np, model, 10)\n",
        "                    if best_score > score:\n",
        "                        best_score = score\n",
        "                        best_params[\"weights\"] = weights\n",
        "                        best_params[\"algorithm\"] = algorithm\n",
        "                        best_params[\"p\"] = p\n",
        "                        best_params[\"leaf_size\"] = leaf_size\n",
        "                        best_params[\"n_neighbors\"] = n_neighbors\n",
        "print(\"best score : \" + str(best_score) + \" avec les best params : \" + str(best_params))\n",
        "KF_KNN = KF_validation(X1_np, Y1_np, KNeighborsRegressor(n_neighbors=2), 10)\n",
        "print(\"KNN without parameters search : \"+ str(KF_KNN))  # 0.19132789047012183\n",
        "\n",
        "### Random Forest parameter opti\n",
        "### best score : 0.1659561008369817 avec les best params : {'n_estimators': 50, 'criterion': 'squared_error', 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 8}\n",
        "### OLD best score : 0.030549093125539557 avec les best params : {'n_estimators': 50, 'criterion': 'poisson', 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
        "# best_score = 1000\n",
        "# best_params = {}\n",
        "# for n_estimators in [50, 100, 150]:\n",
        "#     for criterion in [\"squared_error\" , \"absolute_error\", \"poisson\"]:\n",
        "#         for max_depth in [None, 5, 10, 15, 20]:\n",
        "#             for min_samples_split in [2, 4, 6, 8]:\n",
        "#                 for min_samples_leaf in [2, 5, 8]:\n",
        "#                     model = RandomForestRegressor(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n",
        "#                     score = KF_validation(X1_np, Y1_np, model, 10)\n",
        "#                     if best_score > score:\n",
        "#                         best_score = score\n",
        "#                         best_params[\"n_estimators\"] = n_estimators\n",
        "#                         best_params[\"criterion\"] = criterion\n",
        "#                         best_params[\"max_depth\"] = max_depth\n",
        "#                         best_params[\"min_samples_split\"] = min_samples_split\n",
        "#                         best_params[\"min_samples_leaf\"] = min_samples_leaf\n",
        "# print(\"best score : \" + str(best_score) + \" avec les best params : \" + str(best_params))\n",
        "# KF_RF = KF_validation(X1_np, Y1_np, RandomForestRegressor(max_depth=2, random_state=1), 10)\n",
        "# print(\"RandomForest without parameters search : \"+ str(KF_RF))  # 0.24233884509903833"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score : 0.124841313131318 avec les best params : {'weights': 'uniform', 'algorithm': 'kd_tree', 'p': 1, 'leaf_size': 10, 'n_neighbors': 15}\n",
            "KNN without parameters search : 0.19132789047012183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "def conf_matrix_printer(bmi_pred, bmi_true, low, high):\n",
        "    tol=1\n",
        "    vpred = (bmi_pred >= low-tol) & (bmi_pred < high+tol)\n",
        "    vtrue = (bmi_true >= low) & (bmi_true < high)\n",
        "\n",
        "    return confusion_matrix(vtrue, vpred)\n",
        "\n",
        "def conf(ytrue, ypred, height):\n",
        "    bmi_pred = ypred/(height*height)\n",
        "    bmi_true = ytrue/(height*height)\n",
        "\n",
        "    conf_matrices = []\n",
        "    for bmi_low, bmi_high in zip([0, 18.5, 25, 30], [18.5, 25, 30, 100]):\n",
        "        conf_matrices.append(conf_matrix_printer(bmi_pred, bmi_true, low=bmi_low, high=bmi_high))\n",
        "    return conf_matrices\n",
        "\n",
        "def KF_confusion(X, Y, model, K, mse=2):\n",
        "    KF = KFold(n_splits=K, random_state=1, shuffle=True)\n",
        "\n",
        "    conf_matrices = []\n",
        "    for train_index, test_index in KF.split(X):\n",
        "        X_train, X_test = X.loc[train_index][:].drop([\"bmi\"], axis=1).to_numpy(), X.loc[test_index][:].drop([\"bmi\"], axis=1).to_numpy()\n",
        "        Y_train, Y_test = Y.loc[train_index][:].drop([\"bmi\"], axis=1).to_numpy(), Y.loc[test_index][:].drop([\"bmi\"], axis=1).to_numpy()\n",
        "\n",
        "        model.fit(X_train, np.ravel(Y_train))\n",
        "        # model.fit(X_train, Y_train)\n",
        "        conf_matrices.append(conf(np.ravel(Y_test), model.predict(X_test), X_test[:,2])) # K element containing 4 conf_matrices (1 per bmi_class)\n",
        "        # conf_matrices.append(conf(Y_test, model.predict(X_test), X_test[:,2])) # K element containing 4 conf_matrices (1 per bmi_class)\n",
        "        # now we do the mean of those 10 element to obtain the final 4 matrices\n",
        "\n",
        "    conf_matrix_low = [[0, 0], [0, 0]]\n",
        "    conf_matrix_normal = [[0, 0], [0, 0]]\n",
        "    conf_matrix_high = [[0, 0], [0, 0]]\n",
        "    conf_matrix_very_high = [[0, 0], [0, 0]]\n",
        "    for split in conf_matrices:\n",
        "        conf_matrix_low = np.add(conf_matrix_low, split[0])\n",
        "        conf_matrix_normal = np.add(conf_matrix_normal, split[1])\n",
        "        conf_matrix_high = np.add(conf_matrix_high, split[2])\n",
        "        conf_matrix_very_high = np.add(conf_matrix_very_high, split[3])\n",
        "\n",
        "    plot_conf_matrix(conf_matrix_low)\n",
        "    plot_conf_matrix(conf_matrix_normal)\n",
        "    plot_conf_matrix(conf_matrix_high)\n",
        "    plot_conf_matrix(conf_matrix_very_high)\n",
        "\n",
        "    return\n",
        "\n",
        "def plot_conf_matrix(matrix):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.matshow(matrix, cmap='tab20b')\n",
        "    # https://matplotlib.org/stable/tutorials/colors/colormaps.html pour les colormaps (incroyable découverte)\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            text = ax.text(j, i, matrix[i, j], ha=\"center\", va=\"center\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "KF_confusion(X1_np, Y1_np, LinearRegression(), 10)\n"
      ],
      "metadata": {
        "id": "rNmqAlyFtu78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bar_plot(tab1, tab2, labels):\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.35  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x - width/2, tab1, width, label='Non-optimized', color='forestgreen')\n",
        "    rects2 = ax.bar(x + width/2, tab2, width, label='Optimized', color='darkviolet')\n",
        "\n",
        "    ax.set_ylabel('Evaluation')\n",
        "    ax.set_title('Evaluation by optimization and model')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "labels = [] # Name of models\n",
        "nopti = []  # Values for Non-optimzed\n",
        "opti = []   # Values for optimized\n",
        "\n",
        "# BarPlot where preprocessing is already done\n",
        "labels = [\"LinearReg\", \"MLP\", \"KNN\", \"RandomForest\"] # Name of models\n",
        "nopti = [0.10152731927116092, 0.169157591378042, 0.19132789047012183, 0.1711198064639973] # Values for Non-optimzed\n",
        "opti = [0.10152731927116092, 0.15898470190027653, 0.124841313131318 , 0.1659561008369817] # Values for optimized\n",
        "bar_plot(nopti, opti, labels)"
      ],
      "metadata": {
        "id": "cCjtIBwjQtB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHJQ0qUUUCm"
      },
      "source": [
        "# Prediction on Y2\n",
        "# one line per prediction and no header, no quotation marks around your numbers either.\n",
        "# At the tail end of this file you will add an additional number which is the estimated performance of your model\n",
        "# on the unseen data.\n",
        "\n",
        "Y1 = df.loc[:][\"weight\"]\n",
        "X1 = df.drop([\"weight\", \"bmi\"], axis=1)\n",
        "\n",
        "estimate = 0.11\n",
        "Y2_LR = LinearRegression().fit(X1, Y1).predict(X2)\n",
        "Y2_LR = np.append(Y2_LR, estimate)\n",
        "Y2_LR = np.around(Y2_LR, 1)\n",
        "prediction = pd.DataFrame(Y2_LR).to_csv('Y2.csv', index=False, header=False)\n",
        "\n",
        "# estimate = 0\n",
        "# Y2_MLP = MLPRegressor(random_state=1, max_iter=1000).fit(X1, Y1).predict(X2)\n",
        "# Y2_MLP = np.append(Y2_MLP, estimate)\n",
        "# Y2_MLP = np.around(Y2_MLP, 1)\n",
        "# prediction = pd.DataFrame(Y2_MLP).to_csv('Y2.csv', index=False, header=False)\n",
        "\n",
        "# estimate = 0\n",
        "# Y2_KNN = KNeighborsRegressor(n_neighbors=10).fit(X1, Y1).predict(X2)\n",
        "# Y2_KNN = np.append(Y2_KNN, estimate)\n",
        "# Y2_KNN = np.around(Y2_KNN, 1)\n",
        "# prediction = pd.DataFrame(Y2_KNN).to_csv('Y2.csv', index=False, header=False)\n",
        "\n",
        "# estimate = 0\n",
        "# Y2_RF = RandomForestRegressor(max_depth=2, random_state=1).fit(X1, Y1).predict(X2)\n",
        "# Y2_RF = np.append(Y2_RF, estimate)\n",
        "# Y2_RF = np.around(Y2_RF, 1)\n",
        "# prediction = pd.DataFrame(Y2_RF).to_csv('Y2.csv', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}